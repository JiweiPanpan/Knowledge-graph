{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load raw dataset from ORKG statements\n",
    "Clean statements and eliminate illegal characters\n",
    "Split triples to train, test, valid .txt file\n",
    "'''\n",
    "from orkg import ORKG\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "orkg = ORKG(host=\"https://orkg.org\", creds=(\"zihao.wang@ipvs.uni-stuttgart.de\", \"HM8eZVFFPMAtXR6\"))\n",
    "\n",
    "response = orkg.resources.get_unpaginated(start_page=1, end_page=100)\n",
    "\n",
    "'''\n",
    "print(response.all_succeeded)\n",
    "print(len(response.responses))\n",
    "print(len(response.content))\n",
    "print(response.content[:3])\n",
    "'''\n",
    "literal = orkg.literals.by_id(id=\"A1\")\n",
    "\n",
    "\n",
    "ent_by_id = orkg.statements.get(size=5000, sort='id', desc=True)\n",
    "print(ent_by_id.content[:5000])\n",
    "\n",
    "data = []\n",
    "for item in ent_by_id.content:\n",
    "    try:\n",
    "        subject = item['subject']['label']\n",
    "        predicate = item['predicate']['label']\n",
    "        obj = item['object']['label']\n",
    "        if \"\\n\" in subject or \"\\t\" in subject or \"\\n\" in predicate or \"\\t\" in predicate or \"\\n\" in obj or \"\\t\" in obj:\n",
    "            continue\n",
    "        if \"path\" in subject or \"path\" in predicate or \"path\" in obj:\n",
    "            continue\n",
    "        data.append((subject, predicate, obj))\n",
    "\n",
    "    except (KeyError, TypeError):\n",
    "        pass\n",
    "\n",
    "\n",
    "def split_dataset(data, train_ratio, valid_ratio, test_ratio):\n",
    "    random.shuffle(data)\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    valid_size = int(len(data) * valid_ratio)\n",
    "\n",
    "    train_data = data[:train_size]\n",
    "    valid_data = data[train_size:train_size + valid_size]\n",
    "    test_data = data[train_size + valid_size:]\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = split_dataset(data, train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1)\n",
    "\n",
    "\n",
    "\n",
    "def remove_illegal_chars(text):\n",
    "    # 定义非法字符的正则表达式模式\n",
    "    illegal_chars_pattern = r\"[^\\w\\s]\"  # 匹配非字母、非数字、非下划线、非空白字符\n",
    "\n",
    "    # 使用正则表达式替换非法字符为空字符串\n",
    "    cleaned_text = re.sub(illegal_chars_pattern, \"\", text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 保存训练集为txt文件\n",
    "train_file_path = 'C://Python//learn_torch//dataset//train.txt'\n",
    "with open(train_file_path, 'w', encoding='utf-8') as train_file:\n",
    "    for item in train_data:\n",
    "\n",
    "            subject = remove_illegal_chars(item[0])\n",
    "            predicate = remove_illegal_chars(item[1])\n",
    "            obj = remove_illegal_chars(item[2])\n",
    "            train_file.write(f\"{subject}\\t{predicate}\\t{obj}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 保存验证集为txt文件\n",
    "valid_file_path = 'dataset/valid.txt'\n",
    "with open(valid_file_path, 'w', encoding='utf-8') as valid_file:\n",
    "    for item in valid_data:\n",
    "\n",
    "            subject = remove_illegal_chars(item[0])\n",
    "            predicate = remove_illegal_chars(item[1])\n",
    "            obj = remove_illegal_chars(item[2])\n",
    "            valid_file.write(f\"{subject}\\t{predicate}\\t{obj}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 保存测试集为txt文件\n",
    "test_file_path = 'dataset/test.txt'\n",
    "with open(test_file_path, 'w', encoding='utf-8') as test_file:\n",
    "    for item in test_data:\n",
    "\n",
    "            subject = remove_illegal_chars(item[0])\n",
    "            predicate = remove_illegal_chars(item[1])\n",
    "            obj = remove_illegal_chars(item[2])\n",
    "            test_file.write(f\"{subject}\\t{predicate}\\t{obj}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "with open('dataset//train.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "cleaned_lines = []\n",
    "incomplete_lines = []\n",
    "for i, line in enumerate(lines):\n",
    "    fields = line.strip().split('\\t')\n",
    "    if len(fields) == 3 and all(fields):\n",
    "        cleaned_lines.append(line)\n",
    "    else:\n",
    "        incomplete_lines.append(i+1)\n",
    "\n",
    "with open('dataset//train.txt', 'w', encoding='utf-8') as new_file:\n",
    "    for line in cleaned_lines:\n",
    "        new_file.write(line)\n",
    "\n",
    "print(\"不完整的三元组行行数：\", incomplete_lines)\n",
    "\n",
    "with open('dataset//test.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "cleaned_lines = []\n",
    "incomplete_lines = []\n",
    "for i, line in enumerate(lines):\n",
    "    fields = line.strip().split('\\t')\n",
    "    if len(fields) == 3 and all(fields):\n",
    "        cleaned_lines.append(line)\n",
    "    else:\n",
    "        incomplete_lines.append(i+1)\n",
    "\n",
    "with open('dataset//test.txt', 'w', encoding='utf-8') as new_file:\n",
    "    for line in cleaned_lines:\n",
    "        new_file.write(line)\n",
    "\n",
    "print(\"不完整的三元组行行数：\", incomplete_lines)\n",
    "\n",
    "\n",
    "\n",
    "with open('dataset//valid.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "cleaned_lines = []\n",
    "incomplete_lines = []\n",
    "for i, line in enumerate(lines):\n",
    "    fields = line.strip().split('\\t')\n",
    "    if len(fields) == 3 and all(fields):\n",
    "        cleaned_lines.append(line)\n",
    "    else:\n",
    "        incomplete_lines.append(i+1)\n",
    "\n",
    "with open('dataset//valid.txt', 'w', encoding='utf-8') as new_file:\n",
    "    for line in cleaned_lines:\n",
    "        new_file.write(line)\n",
    "\n",
    "print(\"不完整的三元组行行数：\", incomplete_lines)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
