{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/valid.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16468\\956460125.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     85\u001B[0m \u001B[1;31m# 保存验证集为txt文件\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[0mvalid_file_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'dataset/valid.txt'\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 87\u001B[1;33m \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalid_file_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'w'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'utf-8'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mvalid_file\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     88\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mvalid_data\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'dataset/valid.txt'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load raw dataset from ORKG statements\n",
    "Clean statements and eliminate illegal characters\n",
    "Split triples to train, test, valid .txt file\n",
    "'''\n",
    "from orkg import ORKG\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "orkg = ORKG(host=\"https://orkg.org\", creds=(\"zihao.wang@ipvs.uni-stuttgart.de\", \"HM8eZVFFPMAtXR6\"))\n",
    "\n",
    "response = orkg.resources.get_unpaginated(start_page=1, end_page=100)\n",
    "\n",
    "'''\n",
    "print(response.all_succeeded)\n",
    "print(len(response.responses))\n",
    "print(len(response.content))\n",
    "print(response.content[:3])\n",
    "'''\n",
    "literal = orkg.literals.by_id(id=\"A1\")\n",
    "\n",
    "\n",
    "ent_by_id = orkg.statements.get(size=5000, sort='id', desc=True)\n",
    "print(ent_by_id.content[:5000])\n",
    "\n",
    "data = []\n",
    "for item in ent_by_id.content:\n",
    "    try:\n",
    "        subject = item['subject']['label']\n",
    "        predicate = item['predicate']['label']\n",
    "        obj = item['object']['label']\n",
    "        if \"\\n\" in subject or \"\\t\" in subject or \"\\n\" in predicate or \"\\t\" in predicate or \"\\n\" in obj or \"\\t\" in obj:\n",
    "            continue\n",
    "        if \"path\" in subject or \"path\" in predicate or \"path\" in obj:\n",
    "            continue\n",
    "        data.append((subject, predicate, obj))\n",
    "\n",
    "    except (KeyError, TypeError):\n",
    "        pass\n",
    "\n",
    "\n",
    "def split_dataset(data, train_ratio, valid_ratio, test_ratio):\n",
    "    random.shuffle(data)\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    valid_size = int(len(data) * valid_ratio)\n",
    "\n",
    "    train_data = data[:train_size]\n",
    "    valid_data = data[train_size:train_size + valid_size]\n",
    "    test_data = data[train_size + valid_size:]\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = split_dataset(data, train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1)\n",
    "\n",
    "\n",
    "\n",
    "def remove_illegal_chars(text):\n",
    "    # 定义非法字符的正则表达式模式\n",
    "    illegal_chars_pattern = r\"[^\\w\\s]\"  # 匹配非字母、非数字、非下划线、非空白字符\n",
    "\n",
    "    # 使用正则表达式替换非法字符为空字符串\n",
    "    cleaned_text = re.sub(illegal_chars_pattern, \"\", text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 保存训练集为txt文件\n",
    "train_file_path = 'C://Python//learn_torch//dataset//train.txt'\n",
    "with open(train_file_path, 'w', encoding='utf-8') as train_file:\n",
    "    for item in train_data:\n",
    "\n",
    "            subject = remove_illegal_chars(item[0])\n",
    "            predicate = remove_illegal_chars(item[1])\n",
    "            obj = remove_illegal_chars(item[2])\n",
    "            train_file.write(f\"{subject}\\t{predicate}\\t{obj}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 保存验证集为txt文件\n",
    "valid_file_path = 'dataset/valid.txt'\n",
    "with open(valid_file_path, 'w', encoding='utf-8') as valid_file:\n",
    "    for item in valid_data:\n",
    "\n",
    "            subject = remove_illegal_chars(item[0])\n",
    "            predicate = remove_illegal_chars(item[1])\n",
    "            obj = remove_illegal_chars(item[2])\n",
    "            valid_file.write(f\"{subject}\\t{predicate}\\t{obj}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 保存测试集为txt文件\n",
    "test_file_path = 'dataset/test.txt'\n",
    "with open(test_file_path, 'w', encoding='utf-8') as test_file:\n",
    "    for item in test_data:\n",
    "\n",
    "            subject = remove_illegal_chars(item[0])\n",
    "            predicate = remove_illegal_chars(item[1])\n",
    "            obj = remove_illegal_chars(item[2])\n",
    "            test_file.write(f\"{subject}\\t{predicate}\\t{obj}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "with open('dataset//train.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "cleaned_lines = []\n",
    "incomplete_lines = []\n",
    "for i, line in enumerate(lines):\n",
    "    fields = line.strip().split('\\t')\n",
    "    if len(fields) == 3 and all(fields):\n",
    "        cleaned_lines.append(line)\n",
    "    else:\n",
    "        incomplete_lines.append(i+1)\n",
    "\n",
    "with open('dataset//train.txt', 'w', encoding='utf-8') as new_file:\n",
    "    for line in cleaned_lines:\n",
    "        new_file.write(line)\n",
    "\n",
    "print(\"不完整的三元组行行数：\", incomplete_lines)\n",
    "\n",
    "with open('dataset//test.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "cleaned_lines = []\n",
    "incomplete_lines = []\n",
    "for i, line in enumerate(lines):\n",
    "    fields = line.strip().split('\\t')\n",
    "    if len(fields) == 3 and all(fields):\n",
    "        cleaned_lines.append(line)\n",
    "    else:\n",
    "        incomplete_lines.append(i+1)\n",
    "\n",
    "with open('dataset//test.txt', 'w', encoding='utf-8') as new_file:\n",
    "    for line in cleaned_lines:\n",
    "        new_file.write(line)\n",
    "\n",
    "print(\"不完整的三元组行行数：\", incomplete_lines)\n",
    "\n",
    "\n",
    "\n",
    "with open('dataset//valid.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "cleaned_lines = []\n",
    "incomplete_lines = []\n",
    "for i, line in enumerate(lines):\n",
    "    fields = line.strip().split('\\t')\n",
    "    if len(fields) == 3 and all(fields):\n",
    "        cleaned_lines.append(line)\n",
    "    else:\n",
    "        incomplete_lines.append(i+1)\n",
    "\n",
    "with open('dataset//valid.txt', 'w', encoding='utf-8') as new_file:\n",
    "    for line in cleaned_lines:\n",
    "        new_file.write(line)\n",
    "\n",
    "print(\"不完整的三元组行行数：\", incomplete_lines)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
